// ‚ö° RAWN PRO ‚Äî API Chat Inteligente com Roteamento Autom√°tico + Streaming + Performance Boost
import OpenAI from "openai";
import { NextResponse } from "next/server";

const client = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

// Fun√ß√£o auxiliar: identifica se o prompt √© complexo
function isComplexPrompt(text) {
  const len = text.length;
  const complexKeywords = [
    "plano", "semana", "cronograma", "dieta", "avan√ßado",
    "personalizado", "periodiza√ß√£o", "estrat√©gia", "maratona",
    "nutri√ß√£o", "fases", "competi√ß√£o", "progresso", "revis√£o"
  ];
  const hasComplexWords = complexKeywords.some((w) =>
    text.toLowerCase().includes(w)
  );
  return len > 280 || hasComplexWords;
}

// üîÑ Fun√ß√£o principal
export async function POST(req) {
  const start = Date.now();

  try {
    const { messages } = await req.json();
    const lastUserMessage = messages[messages.length - 1]?.content || "";

    // üîç Seleciona modelo dinamicamente
    const useGpt4o = isComplexPrompt(lastUserMessage);
    const model = useGpt4o ? "gpt-4o" : "gpt-4o-mini";

    console.log(`ü§ñ Modelo usado: ${model}`);

    // üî• Streaming de resposta
    const completion = await client.chat.completions.create({
      model,
      messages,
      stream: true,
      temperature: useGpt4o ? 0.7 : 0.6,
      max_tokens: 600,
    });

    const encoder = new TextEncoder();
    const stream = new ReadableStream({
      async start(controller) {
        try {
          for await (const chunk of completion) {
            const token = chunk.choices?.[0]?.delta?.content || "";
            controller.enqueue(encoder.encode(token));
          }
        } catch (err) {
          console.error("‚ö†Ô∏è Erro no stream:", err);
          controller.enqueue(
            encoder.encode("‚ö†Ô∏è Erro ao processar a resposta. Tente novamente.")
          );
        } finally {
          const duration = ((Date.now() - start) / 1000).toFixed(1);
          console.log(`‚úÖ Resposta conclu√≠da em ${duration}s (${model})`);
          controller.close();
        }
      },
    });

    return new NextResponse(stream, {
      headers: {
        "Content-Type": "text/plain; charset=utf-8",
        "Cache-Control": "no-cache, no-transform",
        Connection: "keep-alive",
      },
    });
  } catch (error) {
    console.error("‚ùå Erro geral no endpoint /api/chat:", error);
    return NextResponse.json(
      { error: "Falha ao processar requisi√ß√£o" },
      { status: 500 }
    );
  }
}
