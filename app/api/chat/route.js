// app/api/chat/route.js ‚Äî RAWN PRO ‚ö° Streaming + Pr√©-di√°logo inteligente (sem libs novas)
import OpenAI from "openai";
import { SYSTEM_PROMPT } from "../../lib/system-prompt";

export async function POST(req) {
  try {
    if (!process.env.OPENAI_API_KEY) {
      return new Response("OPENAI_API_KEY n√£o configurada.", { status: 500 });
    }

    const body = await req.json().catch(() => ({}));
    const { messages, message } = body || {};
    const history =
      messages ?? (message ? [{ role: "user", content: String(message) }] : []);

    if (!history.length) {
      return new Response("Nenhuma mensagem recebida.", { status: 400 });
    }

    // üîé Pr√©-di√°logo inteligente
    const last = history[history.length - 1];
    const lastText = (last?.content || "").toLowerCase();
    const isUser = last?.role === "user";

    const gatilhos = [
      "treino",
      "treinamento",
      "planilha",
      "corrida",
      "maratona",
      "muscula√ß√£o",
      "for√ßa",
      "hipertrofia",
      "emagrecimento",
      "dieta",
      "alimenta√ß√£o",
      "plano",
    ];

    const primeiraRespostaDoAssistente =
      history.findIndex((m) => m.role === "assistant") === -1;

    const pedePlano = isUser && gatilhos.some((t) => lastText.includes(t));

    if (primeiraRespostaDoAssistente && pedePlano) {
      const texto =
        "üëã Perfeito! Antes de montar seu plano completo, preciso de **3 informa√ß√µes r√°pidas** para personalizar de verdade:\n\n" +
        "1Ô∏è‚É£ Qual √© o seu **n√≠vel atual** (iniciante, intermedi√°rio ou avan√ßado)?\n" +
        "2Ô∏è‚É£ Quantos **dias/semana** voc√™ consegue treinar?\n" +
        "3Ô∏è‚É£ Qual o **foco principal** agora (for√ßa, resist√™ncia, perda de gordura, ou corrida/maratona) ?\n\n" +
        "Assim que voc√™ responder, eu gero o plano completo com progress√µes, volumes e ajustes.\n" +
        "_‚öôÔ∏è Leva cerca de **20 segundos** porque o RAWN PRO calcula cargas, intensidades e recupera√ß√£o de forma autom√°tica._";

      // Resposta direta (um √∫nico chunk). Seu Chat.js lida bem com isso.
      return new Response(texto, {
        status: 200,
        headers: { "Content-Type": "text/plain; charset=utf-8" },
      });
    }

    // ‚úÖ Resposta normal com streaming (como voc√™ j√° usava)
    const client = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

    const completion = await client.chat.completions.create({
      model: process.env.OPENAI_MODEL || "gpt-4o-mini",
      temperature: 0.7,
      max_tokens: 800,
      stream: true,
      messages: [{ role: "system", content: SYSTEM_PROMPT }, ...history],
    });

    const encoder = new TextEncoder();

    const stream = new ReadableStream({
      async start(controller) {
        try {
          for await (const part of completion) {
            const delta = part.choices?.[0]?.delta?.content;
            if (delta) controller.enqueue(encoder.encode(delta));
          }
        } catch (e) {
          console.error("Stream error:", e);
          controller.enqueue(encoder.encode("‚ö†Ô∏è Erro ao gerar resposta."));
        } finally {
          controller.close();
        }
      },
    });

    return new Response(stream, {
      status: 200,
      headers: { "Content-Type": "text/plain; charset=utf-8" },
    });
  } catch (error) {
    console.error("‚ùå Erro interno na rota /api/chat:", error);
    return new Response("Erro interno no RAWN PRO", { status: 500 });
  }
}
